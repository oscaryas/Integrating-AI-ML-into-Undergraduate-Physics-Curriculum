{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Note: This Notebook severely modified the following notebook:\n",
        "\n",
        "https://github.com/drckf/mlreview_notebooks/blob/master/jupyter_notebooks/notebooks/NB13_CIX-DNN_susy_Pytorch.ipynb\n"
      ],
      "metadata": {
        "id": "3GZGG92aegOq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxwv-CJB-uRb"
      },
      "source": [
        "# **Neural Networks in High Energy Physics**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this module we will be applying deep neural networks to the field of high energy physics. Specifically we will be classifying whether an event can be classified as \"Standard Physics\" (Standard Model Physics) or \"Supersymmetry\" (New Particle Creation Physics)"
      ],
      "metadata": {
        "id": "wLJKzalSARS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Background Science and Dataset Exploration**"
      ],
      "metadata": {
        "id": "eiLgN4JnBAkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Model of Particle Physics"
      ],
      "metadata": {
        "id": "a8CBHAIfCCaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://tikz.net/wp-content/uploads/2024/03/SM_particles-004.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x6BHWygYCEbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Standard Model of particle physics is our best theory for describing how particles interact. It divides particles into two main types: fermions and bosons. Fermions are what make up matter and bosons are what carry force. The Higgs Boson is also a boson but it describes each particles mass."
      ],
      "metadata": {
        "id": "RTZSl2CTCaKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Standard Model of particle physics is extremely successful but doesn't answer all questions. So SUSY (Supersymmetry) is a proposed extension of the Standard Model of particle physics."
      ],
      "metadata": {
        "id": "eaZGCd6DC9_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchy Problem:"
      ],
      "metadata": {
        "id": "8bK-NKWJDvYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Higgs boson is measured to have a weight of $125 \\frac{\\text{GeV}}{\\text{c}^2}$ where the speed of light is set to be a constant of 1. (Note: Standard practice in particle physics calculations is to set speed of light to 1)\n",
        "\n",
        "When we go about calculating loop diagrams, a part of the Feynman diagrams which describe particle interactions in quantum field theory, for how the Higgs Boson interacts with other particle, the calculations suggest that the Higgs mass should be closer to $10^{19} \\frac{\\text{GeV}}{\\text{c}^2}$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PArNaAtbEeZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div>\n",
        "<img src=\"https://www.researchgate.net/publication/331110801/figure/fig2/AS:726420242329600@1550203490295/Feynman-diagrams-of-loop-corrections-to-the-W-boson-propagator.png\" width=\"500\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "nqBP0derYOM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The image above is an example of a loop diagram calculation :*"
      ],
      "metadata": {
        "id": "RvvBfY0sZnEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem is that the actual, measured Higgs mass is tiny compared to that expectation. The only way this can happen in the Standard Model is if the “bare” Higgs mass and the huge loop corrections almost perfectly cancel each other, leaving the small value we observe.\n",
        "\n",
        "Supersymmetry tries to address this issue.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1KLwMyNBZmcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supersymmetric:"
      ],
      "metadata": {
        "id": "6KsEoYHfBURO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://sites.uci.edu/energyobserver/files/2012/12/MSSMBrokenEnglish-1024x812.jpg\" width=\"500\"/>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "byGts0uwBWT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supersymmetry (SUSY) is an extension of the Standard Model of Physics which addresses questions that the Standard Model of Physics can not answer. The proposition is that for each fermion, there is a corresponding boson (i.e. an electron boson, a top quark boson, etc.), and for each boson, there is a corresponding fermion (a gluon fermion)."
      ],
      "metadata": {
        "id": "_cY1eHuTB0Nf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fncomms5308/MediaObjects/41467_2014_Article_BFncomms5308_Fig1_HTML.jpg?as=webp\" width=\"500\"/>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "4X6RNHyrBbWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Diagram describing the signal process involving new exotic Higgs bosons H0 and H±. (b) Diagram describing the background process involving top quarks (t). In both cases, the resulting particles are two W bosons and two b-quarks."
      ],
      "metadata": {
        "id": "kOpqHzCXBh1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Overview"
      ],
      "metadata": {
        "id": "4SpCyGzrCFmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will be classifying the supersymmetry dataset, introduced by first introduced by Baldi et al. in [Nature Communication (2015)](https://www.nature.com/articles/ncomms5308). The SUSY dataset consists of 5,000,000 Monte-Carlo samples of supersymmetric and non-supersymmetric collisions with 18 features.\n",
        "\n"
      ],
      "metadata": {
        "id": "mJEqcWvIDK4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features"
      ],
      "metadata": {
        "id": "B18AefiuBU0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The signal process is the production of electrically-charged supersymmetric particles which decay to $W$ bosons and an electrically-neutral supersymmetric particle that is invisible to the detector.\n",
        "\n",
        "The first $8$ features are \"raw\" kinematic features that can be directly measured from collisions. The final $10$ features are \"hand constructed\" features that have been chosen using physical knowledge and are known to be important in distinguishing supersymmetric and non-supersymmetric collision events. More specifically, they are given by the column names below."
      ],
      "metadata": {
        "id": "Dgrw7AoxBVyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labels"
      ],
      "metadata": {
        "id": "ahvYYpYoBSCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 indicates that a SUSY event has occurred.\n",
        "\n",
        "0 indicates that standard particle interaction occurred"
      ],
      "metadata": {
        "id": "9-EkUD48dNIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install numpy torch pandas scikit-learn"
      ],
      "metadata": {
        "id": "ww2JLESQa-w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKFO8Erj-uRg"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "import os,sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"janus137/supersymmetry-dataset\")\n",
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ncrnGbjqa4pS",
        "outputId": "0d0aa1ba-019b-49d0-cc1f-ea25bd512e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/kaggle/input/supersymmetry-dataset'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zwI_NgvucjPM",
        "outputId": "0334b642-544c-43f0-a9c1-a5e39a604564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/kaggle/input/supersymmetry-dataset'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.patches as mpatches\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "zTMmmWoWcXNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/supersymmetry-dataset/supersymmetry_dataset.csv\")"
      ],
      "metadata": {
        "id": "RKN8WNmqceG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print first 5 rows of df"
      ],
      "metadata": {
        "id": "Rw2B8nToc6tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examine the Data Balance"
      ],
      "metadata": {
        "id": "z5YLz_xgdYdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a bar plot of the SUSY Interaction and Standard Model Interaction\n",
        "#Print out the numbers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "XExAioO-dhBH",
        "outputId": "1b065e24-0707-4852-9388-0a9a90aec74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2287827\n",
            "2712173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you think that is a major data imbalance?"
      ],
      "metadata": {
        "id": "sHY3u7wSdvL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cursory Plots of Features of Data Labels"
      ],
      "metadata": {
        "id": "GD3B1HPWemG7"
      }
    },
    {
      "source": [
        "#Generate a plot between Lepton 1 Pseudorapiditiy (n) vs Lepton 1 Transverse Momentum (pT)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jnMtIbS7gY1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "fig, ax = plt.subplots()\n",
        "first40 = df.head(500)\n",
        "\n",
        "ax.scatter(first40[\"lepton 1 pT\"], first40[\"lepton 2 eta\"],\n",
        "           s=50, c=first40[\"SUSY\"].map({0:\"red\", 1:\"green\"}))\n",
        "\n",
        "ax.set_xlabel(\"Lepton 1 Transverse Momentum (pT)\")\n",
        "ax.set_ylabel(\"Lepton 2 eta (η)\")\n",
        "plt.show()\n",
        "\n",
        "#Generate a plot between Lepton 1 Pseudorapiditiy (n) vs Lepton 1 Transverse Momentum (pT)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bW8Y1bNLgX2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set a Baseline"
      ],
      "metadata": {
        "id": "366vkU1sexMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pick your favorite non-neural network method to do classification. I would suggest a non-linear supervised method"
      ],
      "metadata": {
        "id": "kbjBNT9heyU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train_Test_Split"
      ],
      "metadata": {
        "id": "NBWBpQGXgDql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#What should features be? What should y be?\n",
        "#Use train test split\n",
        "X, y =\n",
        "x_train, x_test, y_train, y_test ="
      ],
      "metadata": {
        "id": "B4SoVjAZduvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What are the shapes you expect for X,y and for your the train test split.?\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vai82bwzjEUz",
        "outputId": "7e450ee3-a2d3-4efd-926d-97f3437906e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4000000, 18)\n",
            "(1000000, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/Fit Model"
      ],
      "metadata": {
        "id": "o1yCBzCxif6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Choose a model and train/fit your model to it."
      ],
      "metadata": {
        "id": "ymE_A0oaiHK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict and Evaluate Model"
      ],
      "metadata": {
        "id": "_Acpw4_Mi5iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict ="
      ],
      "metadata": {
        "id": "uuo6c4sWe4Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score,f1_score, ConfusionMatrixDisplay\n",
        "\n",
        "#Accuracy\n",
        "accuracy = =\n",
        "recall =\n",
        "precision =\n",
        "f1 =\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1: {f1}\")\n",
        "\n",
        "\n",
        "print(\"Training:\")\n",
        "ConfusionMatrixDisplay.from_estimator(model, train_X, train_y, cmap='Blues')\n",
        "plt.show()\n",
        "print(\"Testing:\")\n",
        "ConfusionMatrixDisplay.from_estimator(model, test_X, test_y, cmap='Blues')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TyXK3a_e4o0",
        "outputId": "78219dd3-7764-446b-9102-4bb73a65ffde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.787966\n",
            "Recall: 0.6760502391455059\n",
            "Precision: 0.8286827170786499\n",
            "F1: 0.7446253459651253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq3WWYjL-uRi"
      },
      "source": [
        "# Structure of the Procedure\n",
        "\n",
        "* ***step 1:*** Load and process the data\n",
        "* ***step 2:*** Define the model and its architecture\n",
        "* ***step 3:*** Choose the optimizer and the cost function\n",
        "* ***step 4:*** Train the model\n",
        "* ***step 5:*** Evaluate the model performance on the *unseen* test data\n",
        "* ***step 6:*** Modify the hyperparameters to optimize performance for the specific data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqoyvrs5-uRi"
      },
      "source": [
        "## Step 1: DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader is used to be able to train a model in batches. Instead of a single epoch training the model on the entire dataset, you can train in batches, sometimes it is too computationally heavy."
      ],
      "metadata": {
        "id": "iGYNorQWKk9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want you to also keep track of what potential hyperparameters are. Since in this notebook we will be going through a grid search and finding the \"Best\" hyperparamters."
      ],
      "metadata": {
        "id": "7-2OBWHzov8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch"
      ],
      "metadata": {
        "id": "UwiWHeg2hFNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtx7TwZMjO7Y",
        "outputId": "6318fdcb-9ac5-4b3f-e1b1-e09efd54741c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95000000"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw-2ZGgX-uRi"
      },
      "outputs": [],
      "source": [
        "features = ['SUSY','lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi',\n",
        "'missing energy magnitude', 'missing energy phi', 'MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2',\n",
        "'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n",
        "\n",
        "low_features = ['lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi',\n",
        "      'missing energy magnitude', 'missing energy phi']\n",
        "\n",
        "high_features = ['MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2','S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n",
        "\n",
        "# Define a custom PyTorch Dataset for the SUSY dataset\n",
        "class SUSYDataset(Dataset):\n",
        "    def __init__(self, dataframe, dataset_size, train = True, high_level_features = None):\n",
        "\n",
        "        # Separate input features (X) from labels (y)\n",
        "        self.X =    # drop target column, keep all features\n",
        "        self.y =                # target column = SUSY signal/background label\n",
        "\n",
        "        # Decide which features to use (low-level, high-level, or both)\n",
        "        if high_level_features is None:\n",
        "            print(\"Using both high and low level features\")   d\n",
        "\n",
        "        elif high_level_features is True:\n",
        "            self.X =    # keep only engineered (\"high-level\") physics features\n",
        "            print(\"Using only high level features\")\n",
        "\n",
        "        elif high_level_features is False:\n",
        "            self.X =  # keep only raw (\"low-level\") kinematic features\n",
        "            print(\"Using only low level features\")\n",
        "\n",
        "        # Limit the dataset size to the specified number of rows (Use iloc)\n",
        "        self.X = self.X. # slice features up to dataset_size\n",
        "        self.y = self.y.  # slice labels up to dataset_size\n",
        "\n",
        "        # Split into training and test subsets (80/20 split)\n",
        "        x_train, x_test, y_train, y_test = train_test_split()\n",
        "\n",
        "        # Store tensors for either training or testing\n",
        "        if train:\n",
        "            self.X = torch.tensor(x_train.values, dtype=torch.float32)  # training features → float32 tensor\n",
        "            self.y = torch.tensor(y_train.values, dtype=torch.float32)  # training labels → float32 tensor\n",
        "        else:\n",
        "            self.X = torch.tensor(x_test.values, dtype=torch.float32)   # testing features\n",
        "            self.y = torch.tensor(y_test.values, dtype=torch.float32)   # testing labels\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return total number of samples in this dataset split\n",
        "        return\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get one sample (features + label) by index\n",
        "        features = self.X[idx]\n",
        "        label = self.y[idx]\n",
        "        return features, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dataset Class has three methods you have to implement because they are abstract methods. The __len__ function, the __getitem__ function, and the constructor."
      ],
      "metadata": {
        "id": "aKyAVjc3X986"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(SUSYDataset(df, dataset_size = 1000, high_level_features = True), batch_size = 32)\n",
        "test_loader = DataLoader(SUSYDataset(df, dataset_size = 1000, train = False, high_level_features = True), batch_size = 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuXj8wdNaqBR",
        "outputId": "76377e52-8a9b-4363-e0ef-a099e11996e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using only high level features\n",
            "Using only high level features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAX7r3Th-uRk"
      },
      "source": [
        "## Step 2: Define the Neural Net and its Architecture\n",
        "\n",
        "I want you to create a neural network using nn.Sequential.\n",
        "\n",
        "Then instead of using we will use the forward funciton for wiring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bBXA4Vl-uRl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class neuralNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "  Attributes:\n",
        "  -------------------\n",
        "  network: nn.Sequential\n",
        "        - nn.Linear (input -> 200 hidden units)\n",
        "        - ReLU activation\n",
        "        - Dropout\n",
        "        - Linear (200 -> 100)\n",
        "        - ReLU activation\n",
        "        - Dropout\n",
        "        - Linear (100 -> 2)\n",
        "        - Softmax (over 2 output classes)\n",
        "  \"\"\"\n",
        "  def __init__(self, high_level_feats = None):\n",
        "    super().__init__()\n",
        "\n",
        "    #Implement the network above.\n",
        "    #the first layer depends on what mode we set.\n",
        "    #It should be conditional on the high_level_feats constructor above.\n",
        "    if high_level_feats is None:\n",
        "        fc1 = nn.Linear(18, 200) # all features\n",
        "    elif high_level_feats:\n",
        "        fc1 =                 # low-level only\n",
        "    else:\n",
        "        fc1 =                 # high-level only\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Defines the feed-forward function for the NN.\n",
        "\n",
        "        A backward function is automatically defined using `torch.autograd`\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : autograd.Tensor\n",
        "            input data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        autograd.Tensor\n",
        "            output layer of NN\n",
        "\n",
        "    '''\n",
        "\n",
        "    return self.network(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since you are desgning your custom neural network you can define the attributes.\n",
        "\n",
        "The only thing that matters for designing your neural network is defining\n",
        "\n",
        "1.   Initialization/Constructor\n",
        "2.   Forward Propagation Function\n",
        "\n"
      ],
      "metadata": {
        "id": "gwsnQX54U64y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class model(nn.Module):\n",
        "  \"\"\"\n",
        "  Attributes:\n",
        "  fc1: nn.Linear(input, 200 hidden units)\n",
        "  fc2: nn.Linear(200, 100)\n",
        "  fc3: nn.Linear(100, 2)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,high_level_feats=None):\n",
        "        # inherit attributes and methods of nn.Module\n",
        "        super().__init__()\n",
        "\n",
        "        # an affine operation: y = Wx + b\n",
        "        if high_level_feats is None:\n",
        "            self.fc1 = nn.Linear(18, 200) # all features\n",
        "        elif high_level_feats:\n",
        "            self.fc1 = nn.Linear(10, 200) # low-level only\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(8, 200) # high-level only\n",
        "\n",
        "        self.fc2 = nn.Linear(200, 100) # see forward function for dimensions\n",
        "        self.fc3 = nn.Linear(100, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "        '''Defines the feed-forward function for the NN.\n",
        "\n",
        "        A backward function is automatically defined using `torch.autograd`\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : autograd.Tensor\n",
        "            input data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        autograd.Tensor\n",
        "            output layer of NN\n",
        "\n",
        "        '''\n",
        "\n",
        "        # apply rectified linear unit\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # apply dropout\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "\n",
        "        # apply rectified linear unit\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # apply dropout\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        # apply affine operation fc2\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # soft-max layer\n",
        "        x = F.sigmoid(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "l-fzvU2aLa84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both ways of defining your model architecture is correct.\n",
        "\n",
        "One way defines the layers as attributes and the other defines it within the forward function.\n",
        "\n",
        "Whichever fancies your boat really"
      ],
      "metadata": {
        "id": "m_LdvoTnWlYn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgiFHA0R-uRl"
      },
      "source": [
        "##3+4 Define Loss and Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate your model\n",
        "\n",
        "#Create a criterion\n",
        "\n",
        "#Choose your optimizer\n"
      ],
      "metadata": {
        "id": "KH1FoCk-iTR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural.train()\n",
        "EPOCHS = #Pick epochs amount\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss= 0\n",
        "\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "      #Set optimizer so it won't accumulate gradients\n",
        "\n",
        "      # compute output of final layer: forward step\n",
        "\n",
        "      # compute loss\n",
        "\n",
        "      # run backprop: backward step\n",
        "\n",
        "      # update weigths of NN\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Avg Loss: {epoch_loss/len(train_loader):.6f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RqOfbPtFY9NT",
        "outputId": "bc1a1a89-7fa4-433b-cbb8-9ffb4b232774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Avg Loss: 0.698766\n",
            "Epoch 2/1000, Avg Loss: 0.694184\n",
            "Epoch 3/1000, Avg Loss: 0.688569\n",
            "Epoch 4/1000, Avg Loss: 0.684350\n",
            "Epoch 5/1000, Avg Loss: 0.683088\n",
            "Epoch 6/1000, Avg Loss: 0.676888\n",
            "Epoch 7/1000, Avg Loss: 0.677396\n",
            "Epoch 8/1000, Avg Loss: 0.677199\n",
            "Epoch 9/1000, Avg Loss: 0.676351\n",
            "Epoch 10/1000, Avg Loss: 0.666492\n",
            "Epoch 11/1000, Avg Loss: 0.669959\n",
            "Epoch 12/1000, Avg Loss: 0.658393\n",
            "Epoch 13/1000, Avg Loss: 0.666937\n",
            "Epoch 14/1000, Avg Loss: 0.661160\n",
            "Epoch 15/1000, Avg Loss: 0.662103\n",
            "Epoch 16/1000, Avg Loss: 0.653938\n",
            "Epoch 17/1000, Avg Loss: 0.653906\n",
            "Epoch 18/1000, Avg Loss: 0.645869\n",
            "Epoch 19/1000, Avg Loss: 0.650201\n",
            "Epoch 20/1000, Avg Loss: 0.643136\n",
            "Epoch 21/1000, Avg Loss: 0.635023\n",
            "Epoch 22/1000, Avg Loss: 0.646115\n",
            "Epoch 23/1000, Avg Loss: 0.647083\n",
            "Epoch 24/1000, Avg Loss: 0.637963\n",
            "Epoch 25/1000, Avg Loss: 0.636119\n",
            "Epoch 26/1000, Avg Loss: 0.633751\n",
            "Epoch 27/1000, Avg Loss: 0.628281\n",
            "Epoch 28/1000, Avg Loss: 0.630618\n",
            "Epoch 29/1000, Avg Loss: 0.616979\n",
            "Epoch 30/1000, Avg Loss: 0.635590\n",
            "Epoch 31/1000, Avg Loss: 0.620550\n",
            "Epoch 32/1000, Avg Loss: 0.618269\n",
            "Epoch 33/1000, Avg Loss: 0.616433\n",
            "Epoch 34/1000, Avg Loss: 0.614720\n",
            "Epoch 35/1000, Avg Loss: 0.617387\n",
            "Epoch 36/1000, Avg Loss: 0.604923\n",
            "Epoch 37/1000, Avg Loss: 0.604205\n",
            "Epoch 38/1000, Avg Loss: 0.613229\n",
            "Epoch 39/1000, Avg Loss: 0.620671\n",
            "Epoch 40/1000, Avg Loss: 0.600145\n",
            "Epoch 41/1000, Avg Loss: 0.608047\n",
            "Epoch 42/1000, Avg Loss: 0.595023\n",
            "Epoch 43/1000, Avg Loss: 0.597879\n",
            "Epoch 44/1000, Avg Loss: 0.598090\n",
            "Epoch 45/1000, Avg Loss: 0.582206\n",
            "Epoch 46/1000, Avg Loss: 0.587398\n",
            "Epoch 47/1000, Avg Loss: 0.598444\n",
            "Epoch 48/1000, Avg Loss: 0.589658\n",
            "Epoch 49/1000, Avg Loss: 0.576327\n",
            "Epoch 50/1000, Avg Loss: 0.583654\n",
            "Epoch 51/1000, Avg Loss: 0.579924\n",
            "Epoch 52/1000, Avg Loss: 0.579074\n",
            "Epoch 53/1000, Avg Loss: 0.576282\n",
            "Epoch 54/1000, Avg Loss: 0.576632\n",
            "Epoch 55/1000, Avg Loss: 0.574638\n",
            "Epoch 56/1000, Avg Loss: 0.578366\n",
            "Epoch 57/1000, Avg Loss: 0.572803\n",
            "Epoch 58/1000, Avg Loss: 0.569439\n",
            "Epoch 59/1000, Avg Loss: 0.575813\n",
            "Epoch 60/1000, Avg Loss: 0.563242\n",
            "Epoch 61/1000, Avg Loss: 0.568615\n",
            "Epoch 62/1000, Avg Loss: 0.567483\n",
            "Epoch 63/1000, Avg Loss: 0.563537\n",
            "Epoch 64/1000, Avg Loss: 0.571966\n",
            "Epoch 65/1000, Avg Loss: 0.559514\n",
            "Epoch 66/1000, Avg Loss: 0.564729\n",
            "Epoch 67/1000, Avg Loss: 0.563111\n",
            "Epoch 68/1000, Avg Loss: 0.564274\n",
            "Epoch 69/1000, Avg Loss: 0.554383\n",
            "Epoch 70/1000, Avg Loss: 0.566970\n",
            "Epoch 71/1000, Avg Loss: 0.561480\n",
            "Epoch 72/1000, Avg Loss: 0.556372\n",
            "Epoch 73/1000, Avg Loss: 0.565175\n",
            "Epoch 74/1000, Avg Loss: 0.554743\n",
            "Epoch 75/1000, Avg Loss: 0.554639\n",
            "Epoch 76/1000, Avg Loss: 0.550082\n",
            "Epoch 77/1000, Avg Loss: 0.554678\n",
            "Epoch 78/1000, Avg Loss: 0.537495\n",
            "Epoch 79/1000, Avg Loss: 0.546686\n",
            "Epoch 80/1000, Avg Loss: 0.532907\n",
            "Epoch 81/1000, Avg Loss: 0.528305\n",
            "Epoch 82/1000, Avg Loss: 0.542361\n",
            "Epoch 83/1000, Avg Loss: 0.556558\n",
            "Epoch 84/1000, Avg Loss: 0.546820\n",
            "Epoch 85/1000, Avg Loss: 0.535421\n",
            "Epoch 86/1000, Avg Loss: 0.535774\n",
            "Epoch 87/1000, Avg Loss: 0.540507\n",
            "Epoch 88/1000, Avg Loss: 0.531874\n",
            "Epoch 89/1000, Avg Loss: 0.537369\n",
            "Epoch 90/1000, Avg Loss: 0.537383\n",
            "Epoch 91/1000, Avg Loss: 0.533374\n",
            "Epoch 92/1000, Avg Loss: 0.535626\n",
            "Epoch 93/1000, Avg Loss: 0.545788\n",
            "Epoch 94/1000, Avg Loss: 0.534069\n",
            "Epoch 95/1000, Avg Loss: 0.524340\n",
            "Epoch 96/1000, Avg Loss: 0.533377\n",
            "Epoch 97/1000, Avg Loss: 0.511829\n",
            "Epoch 98/1000, Avg Loss: 0.530638\n",
            "Epoch 99/1000, Avg Loss: 0.533408\n",
            "Epoch 100/1000, Avg Loss: 0.532957\n",
            "Epoch 101/1000, Avg Loss: 0.534152\n",
            "Epoch 102/1000, Avg Loss: 0.530683\n",
            "Epoch 103/1000, Avg Loss: 0.522564\n",
            "Epoch 104/1000, Avg Loss: 0.531331\n",
            "Epoch 105/1000, Avg Loss: 0.524973\n",
            "Epoch 106/1000, Avg Loss: 0.522831\n",
            "Epoch 107/1000, Avg Loss: 0.513861\n",
            "Epoch 108/1000, Avg Loss: 0.513836\n",
            "Epoch 109/1000, Avg Loss: 0.517610\n",
            "Epoch 110/1000, Avg Loss: 0.517628\n",
            "Epoch 111/1000, Avg Loss: 0.536281\n",
            "Epoch 112/1000, Avg Loss: 0.517386\n",
            "Epoch 113/1000, Avg Loss: 0.519871\n",
            "Epoch 114/1000, Avg Loss: 0.514902\n",
            "Epoch 115/1000, Avg Loss: 0.519752\n",
            "Epoch 116/1000, Avg Loss: 0.519204\n",
            "Epoch 117/1000, Avg Loss: 0.514992\n",
            "Epoch 118/1000, Avg Loss: 0.517375\n",
            "Epoch 119/1000, Avg Loss: 0.514090\n",
            "Epoch 120/1000, Avg Loss: 0.518216\n",
            "Epoch 121/1000, Avg Loss: 0.501780\n",
            "Epoch 122/1000, Avg Loss: 0.516977\n",
            "Epoch 123/1000, Avg Loss: 0.510070\n",
            "Epoch 124/1000, Avg Loss: 0.509390\n",
            "Epoch 125/1000, Avg Loss: 0.520203\n",
            "Epoch 126/1000, Avg Loss: 0.525713\n",
            "Epoch 127/1000, Avg Loss: 0.513152\n",
            "Epoch 128/1000, Avg Loss: 0.499489\n",
            "Epoch 129/1000, Avg Loss: 0.507833\n",
            "Epoch 130/1000, Avg Loss: 0.505909\n",
            "Epoch 131/1000, Avg Loss: 0.503988\n",
            "Epoch 132/1000, Avg Loss: 0.516634\n",
            "Epoch 133/1000, Avg Loss: 0.506237\n",
            "Epoch 134/1000, Avg Loss: 0.511768\n",
            "Epoch 135/1000, Avg Loss: 0.502908\n",
            "Epoch 136/1000, Avg Loss: 0.490797\n",
            "Epoch 137/1000, Avg Loss: 0.498510\n",
            "Epoch 138/1000, Avg Loss: 0.493631\n",
            "Epoch 139/1000, Avg Loss: 0.494210\n",
            "Epoch 140/1000, Avg Loss: 0.507138\n",
            "Epoch 141/1000, Avg Loss: 0.493349\n",
            "Epoch 142/1000, Avg Loss: 0.512623\n",
            "Epoch 143/1000, Avg Loss: 0.504389\n",
            "Epoch 144/1000, Avg Loss: 0.498614\n",
            "Epoch 145/1000, Avg Loss: 0.496769\n",
            "Epoch 146/1000, Avg Loss: 0.500473\n",
            "Epoch 147/1000, Avg Loss: 0.510935\n",
            "Epoch 148/1000, Avg Loss: 0.502994\n",
            "Epoch 149/1000, Avg Loss: 0.501399\n",
            "Epoch 150/1000, Avg Loss: 0.495170\n",
            "Epoch 151/1000, Avg Loss: 0.485654\n",
            "Epoch 152/1000, Avg Loss: 0.495419\n",
            "Epoch 153/1000, Avg Loss: 0.509444\n",
            "Epoch 154/1000, Avg Loss: 0.484504\n",
            "Epoch 155/1000, Avg Loss: 0.498771\n",
            "Epoch 156/1000, Avg Loss: 0.506850\n",
            "Epoch 157/1000, Avg Loss: 0.495279\n",
            "Epoch 158/1000, Avg Loss: 0.501698\n",
            "Epoch 159/1000, Avg Loss: 0.514101\n",
            "Epoch 160/1000, Avg Loss: 0.489964\n",
            "Epoch 161/1000, Avg Loss: 0.490932\n",
            "Epoch 162/1000, Avg Loss: 0.483491\n",
            "Epoch 163/1000, Avg Loss: 0.491597\n",
            "Epoch 164/1000, Avg Loss: 0.494757\n",
            "Epoch 165/1000, Avg Loss: 0.489778\n",
            "Epoch 166/1000, Avg Loss: 0.489247\n",
            "Epoch 167/1000, Avg Loss: 0.474748\n",
            "Epoch 168/1000, Avg Loss: 0.489318\n",
            "Epoch 169/1000, Avg Loss: 0.497442\n",
            "Epoch 170/1000, Avg Loss: 0.484012\n",
            "Epoch 171/1000, Avg Loss: 0.497836\n",
            "Epoch 172/1000, Avg Loss: 0.485825\n",
            "Epoch 173/1000, Avg Loss: 0.504292\n",
            "Epoch 174/1000, Avg Loss: 0.492471\n",
            "Epoch 175/1000, Avg Loss: 0.494508\n",
            "Epoch 176/1000, Avg Loss: 0.496455\n",
            "Epoch 177/1000, Avg Loss: 0.489888\n",
            "Epoch 178/1000, Avg Loss: 0.478229\n",
            "Epoch 179/1000, Avg Loss: 0.480999\n",
            "Epoch 180/1000, Avg Loss: 0.494029\n",
            "Epoch 181/1000, Avg Loss: 0.488274\n",
            "Epoch 182/1000, Avg Loss: 0.478367\n",
            "Epoch 183/1000, Avg Loss: 0.487164\n",
            "Epoch 184/1000, Avg Loss: 0.477187\n",
            "Epoch 185/1000, Avg Loss: 0.473192\n",
            "Epoch 186/1000, Avg Loss: 0.487027\n",
            "Epoch 187/1000, Avg Loss: 0.489582\n",
            "Epoch 188/1000, Avg Loss: 0.492117\n",
            "Epoch 189/1000, Avg Loss: 0.481380\n",
            "Epoch 190/1000, Avg Loss: 0.492972\n",
            "Epoch 191/1000, Avg Loss: 0.489319\n",
            "Epoch 192/1000, Avg Loss: 0.488281\n",
            "Epoch 193/1000, Avg Loss: 0.490737\n",
            "Epoch 194/1000, Avg Loss: 0.469071\n",
            "Epoch 195/1000, Avg Loss: 0.484920\n",
            "Epoch 196/1000, Avg Loss: 0.481543\n",
            "Epoch 197/1000, Avg Loss: 0.471049\n",
            "Epoch 198/1000, Avg Loss: 0.477420\n",
            "Epoch 199/1000, Avg Loss: 0.475202\n",
            "Epoch 200/1000, Avg Loss: 0.486196\n",
            "Epoch 201/1000, Avg Loss: 0.471627\n",
            "Epoch 202/1000, Avg Loss: 0.483854\n",
            "Epoch 203/1000, Avg Loss: 0.485388\n",
            "Epoch 204/1000, Avg Loss: 0.471173\n",
            "Epoch 205/1000, Avg Loss: 0.477444\n",
            "Epoch 206/1000, Avg Loss: 0.494771\n",
            "Epoch 207/1000, Avg Loss: 0.477280\n",
            "Epoch 208/1000, Avg Loss: 0.486630\n",
            "Epoch 209/1000, Avg Loss: 0.480066\n",
            "Epoch 210/1000, Avg Loss: 0.483715\n",
            "Epoch 211/1000, Avg Loss: 0.477096\n",
            "Epoch 212/1000, Avg Loss: 0.491537\n",
            "Epoch 213/1000, Avg Loss: 0.480093\n",
            "Epoch 214/1000, Avg Loss: 0.483743\n",
            "Epoch 215/1000, Avg Loss: 0.476437\n",
            "Epoch 216/1000, Avg Loss: 0.473694\n",
            "Epoch 217/1000, Avg Loss: 0.463964\n",
            "Epoch 218/1000, Avg Loss: 0.475173\n",
            "Epoch 219/1000, Avg Loss: 0.489967\n",
            "Epoch 220/1000, Avg Loss: 0.476663\n",
            "Epoch 221/1000, Avg Loss: 0.484002\n",
            "Epoch 222/1000, Avg Loss: 0.484983\n",
            "Epoch 223/1000, Avg Loss: 0.476163\n",
            "Epoch 224/1000, Avg Loss: 0.476615\n",
            "Epoch 225/1000, Avg Loss: 0.468811\n",
            "Epoch 226/1000, Avg Loss: 0.470845\n",
            "Epoch 227/1000, Avg Loss: 0.478149\n",
            "Epoch 228/1000, Avg Loss: 0.479466\n",
            "Epoch 229/1000, Avg Loss: 0.479053\n",
            "Epoch 230/1000, Avg Loss: 0.475422\n",
            "Epoch 231/1000, Avg Loss: 0.473618\n",
            "Epoch 232/1000, Avg Loss: 0.477219\n",
            "Epoch 233/1000, Avg Loss: 0.471284\n",
            "Epoch 234/1000, Avg Loss: 0.479699\n",
            "Epoch 235/1000, Avg Loss: 0.457431\n",
            "Epoch 236/1000, Avg Loss: 0.477504\n",
            "Epoch 237/1000, Avg Loss: 0.458221\n",
            "Epoch 238/1000, Avg Loss: 0.469603\n",
            "Epoch 239/1000, Avg Loss: 0.464602\n",
            "Epoch 240/1000, Avg Loss: 0.483864\n",
            "Epoch 241/1000, Avg Loss: 0.472687\n",
            "Epoch 242/1000, Avg Loss: 0.474392\n",
            "Epoch 243/1000, Avg Loss: 0.479226\n",
            "Epoch 244/1000, Avg Loss: 0.479596\n",
            "Epoch 245/1000, Avg Loss: 0.468263\n",
            "Epoch 246/1000, Avg Loss: 0.466343\n",
            "Epoch 247/1000, Avg Loss: 0.481212\n",
            "Epoch 248/1000, Avg Loss: 0.486362\n",
            "Epoch 249/1000, Avg Loss: 0.470358\n",
            "Epoch 250/1000, Avg Loss: 0.461547\n",
            "Epoch 251/1000, Avg Loss: 0.492578\n",
            "Epoch 252/1000, Avg Loss: 0.467328\n",
            "Epoch 253/1000, Avg Loss: 0.477878\n",
            "Epoch 254/1000, Avg Loss: 0.469610\n",
            "Epoch 255/1000, Avg Loss: 0.481550\n",
            "Epoch 256/1000, Avg Loss: 0.481140\n",
            "Epoch 257/1000, Avg Loss: 0.478269\n",
            "Epoch 258/1000, Avg Loss: 0.468129\n",
            "Epoch 259/1000, Avg Loss: 0.462015\n",
            "Epoch 260/1000, Avg Loss: 0.467774\n",
            "Epoch 261/1000, Avg Loss: 0.463365\n",
            "Epoch 262/1000, Avg Loss: 0.484625\n",
            "Epoch 263/1000, Avg Loss: 0.462853\n",
            "Epoch 264/1000, Avg Loss: 0.478063\n",
            "Epoch 265/1000, Avg Loss: 0.473625\n",
            "Epoch 266/1000, Avg Loss: 0.454440\n",
            "Epoch 267/1000, Avg Loss: 0.472406\n",
            "Epoch 268/1000, Avg Loss: 0.468433\n",
            "Epoch 269/1000, Avg Loss: 0.460732\n",
            "Epoch 270/1000, Avg Loss: 0.470743\n",
            "Epoch 271/1000, Avg Loss: 0.451485\n",
            "Epoch 272/1000, Avg Loss: 0.460012\n",
            "Epoch 273/1000, Avg Loss: 0.471836\n",
            "Epoch 274/1000, Avg Loss: 0.456840\n",
            "Epoch 275/1000, Avg Loss: 0.445659\n",
            "Epoch 276/1000, Avg Loss: 0.461312\n",
            "Epoch 277/1000, Avg Loss: 0.469633\n",
            "Epoch 278/1000, Avg Loss: 0.471747\n",
            "Epoch 279/1000, Avg Loss: 0.457907\n",
            "Epoch 280/1000, Avg Loss: 0.467972\n",
            "Epoch 281/1000, Avg Loss: 0.465922\n",
            "Epoch 282/1000, Avg Loss: 0.472016\n",
            "Epoch 283/1000, Avg Loss: 0.464984\n",
            "Epoch 284/1000, Avg Loss: 0.458933\n",
            "Epoch 285/1000, Avg Loss: 0.468485\n",
            "Epoch 286/1000, Avg Loss: 0.455256\n",
            "Epoch 287/1000, Avg Loss: 0.478540\n",
            "Epoch 288/1000, Avg Loss: 0.469511\n",
            "Epoch 289/1000, Avg Loss: 0.474339\n",
            "Epoch 290/1000, Avg Loss: 0.460153\n",
            "Epoch 291/1000, Avg Loss: 0.474600\n",
            "Epoch 292/1000, Avg Loss: 0.460257\n",
            "Epoch 293/1000, Avg Loss: 0.467610\n",
            "Epoch 294/1000, Avg Loss: 0.450637\n",
            "Epoch 295/1000, Avg Loss: 0.474333\n",
            "Epoch 296/1000, Avg Loss: 0.466956\n",
            "Epoch 297/1000, Avg Loss: 0.471902\n",
            "Epoch 298/1000, Avg Loss: 0.446546\n",
            "Epoch 299/1000, Avg Loss: 0.463331\n",
            "Epoch 300/1000, Avg Loss: 0.465227\n",
            "Epoch 301/1000, Avg Loss: 0.456143\n",
            "Epoch 302/1000, Avg Loss: 0.461728\n",
            "Epoch 303/1000, Avg Loss: 0.470821\n",
            "Epoch 304/1000, Avg Loss: 0.459508\n",
            "Epoch 305/1000, Avg Loss: 0.462711\n",
            "Epoch 306/1000, Avg Loss: 0.460296\n",
            "Epoch 307/1000, Avg Loss: 0.479946\n",
            "Epoch 308/1000, Avg Loss: 0.462658\n",
            "Epoch 309/1000, Avg Loss: 0.448613\n",
            "Epoch 310/1000, Avg Loss: 0.453696\n",
            "Epoch 311/1000, Avg Loss: 0.460234\n",
            "Epoch 312/1000, Avg Loss: 0.445605\n",
            "Epoch 313/1000, Avg Loss: 0.467630\n",
            "Epoch 314/1000, Avg Loss: 0.460681\n",
            "Epoch 315/1000, Avg Loss: 0.447730\n",
            "Epoch 316/1000, Avg Loss: 0.459527\n",
            "Epoch 317/1000, Avg Loss: 0.458273\n",
            "Epoch 318/1000, Avg Loss: 0.452303\n",
            "Epoch 319/1000, Avg Loss: 0.461483\n",
            "Epoch 320/1000, Avg Loss: 0.451773\n",
            "Epoch 321/1000, Avg Loss: 0.457706\n",
            "Epoch 322/1000, Avg Loss: 0.466548\n",
            "Epoch 323/1000, Avg Loss: 0.451925\n",
            "Epoch 324/1000, Avg Loss: 0.459888\n",
            "Epoch 325/1000, Avg Loss: 0.471080\n",
            "Epoch 326/1000, Avg Loss: 0.462866\n",
            "Epoch 327/1000, Avg Loss: 0.468933\n",
            "Epoch 328/1000, Avg Loss: 0.460865\n",
            "Epoch 329/1000, Avg Loss: 0.446628\n",
            "Epoch 330/1000, Avg Loss: 0.459090\n",
            "Epoch 331/1000, Avg Loss: 0.456912\n",
            "Epoch 332/1000, Avg Loss: 0.451891\n",
            "Epoch 333/1000, Avg Loss: 0.460231\n",
            "Epoch 334/1000, Avg Loss: 0.454229\n",
            "Epoch 335/1000, Avg Loss: 0.457348\n",
            "Epoch 336/1000, Avg Loss: 0.470196\n",
            "Epoch 337/1000, Avg Loss: 0.454708\n",
            "Epoch 338/1000, Avg Loss: 0.456998\n",
            "Epoch 339/1000, Avg Loss: 0.449512\n",
            "Epoch 340/1000, Avg Loss: 0.447603\n",
            "Epoch 341/1000, Avg Loss: 0.457463\n",
            "Epoch 342/1000, Avg Loss: 0.462502\n",
            "Epoch 343/1000, Avg Loss: 0.449348\n",
            "Epoch 344/1000, Avg Loss: 0.447065\n",
            "Epoch 345/1000, Avg Loss: 0.454855\n",
            "Epoch 346/1000, Avg Loss: 0.466153\n",
            "Epoch 347/1000, Avg Loss: 0.454522\n",
            "Epoch 348/1000, Avg Loss: 0.455281\n",
            "Epoch 349/1000, Avg Loss: 0.461164\n",
            "Epoch 350/1000, Avg Loss: 0.452719\n",
            "Epoch 351/1000, Avg Loss: 0.458398\n",
            "Epoch 352/1000, Avg Loss: 0.452788\n",
            "Epoch 353/1000, Avg Loss: 0.453047\n",
            "Epoch 354/1000, Avg Loss: 0.449180\n",
            "Epoch 355/1000, Avg Loss: 0.455026\n",
            "Epoch 356/1000, Avg Loss: 0.455467\n",
            "Epoch 357/1000, Avg Loss: 0.446622\n",
            "Epoch 358/1000, Avg Loss: 0.473626\n",
            "Epoch 359/1000, Avg Loss: 0.453394\n",
            "Epoch 360/1000, Avg Loss: 0.452253\n",
            "Epoch 361/1000, Avg Loss: 0.458818\n",
            "Epoch 362/1000, Avg Loss: 0.454177\n",
            "Epoch 363/1000, Avg Loss: 0.464753\n",
            "Epoch 364/1000, Avg Loss: 0.449533\n",
            "Epoch 365/1000, Avg Loss: 0.460874\n",
            "Epoch 366/1000, Avg Loss: 0.458626\n",
            "Epoch 367/1000, Avg Loss: 0.450452\n",
            "Epoch 368/1000, Avg Loss: 0.455756\n",
            "Epoch 369/1000, Avg Loss: 0.448283\n",
            "Epoch 370/1000, Avg Loss: 0.462160\n",
            "Epoch 371/1000, Avg Loss: 0.449104\n",
            "Epoch 372/1000, Avg Loss: 0.463475\n",
            "Epoch 373/1000, Avg Loss: 0.454765\n",
            "Epoch 374/1000, Avg Loss: 0.457301\n",
            "Epoch 375/1000, Avg Loss: 0.453278\n",
            "Epoch 376/1000, Avg Loss: 0.461031\n",
            "Epoch 377/1000, Avg Loss: 0.450861\n",
            "Epoch 378/1000, Avg Loss: 0.459085\n",
            "Epoch 379/1000, Avg Loss: 0.461974\n",
            "Epoch 380/1000, Avg Loss: 0.467299\n",
            "Epoch 381/1000, Avg Loss: 0.455307\n",
            "Epoch 382/1000, Avg Loss: 0.473405\n",
            "Epoch 383/1000, Avg Loss: 0.448838\n",
            "Epoch 384/1000, Avg Loss: 0.461433\n",
            "Epoch 385/1000, Avg Loss: 0.448362\n",
            "Epoch 386/1000, Avg Loss: 0.451626\n",
            "Epoch 387/1000, Avg Loss: 0.462721\n",
            "Epoch 388/1000, Avg Loss: 0.462455\n",
            "Epoch 389/1000, Avg Loss: 0.463082\n",
            "Epoch 390/1000, Avg Loss: 0.453658\n",
            "Epoch 391/1000, Avg Loss: 0.441299\n",
            "Epoch 392/1000, Avg Loss: 0.444228\n",
            "Epoch 393/1000, Avg Loss: 0.458049\n",
            "Epoch 394/1000, Avg Loss: 0.467261\n",
            "Epoch 395/1000, Avg Loss: 0.464445\n",
            "Epoch 396/1000, Avg Loss: 0.461177\n",
            "Epoch 397/1000, Avg Loss: 0.457311\n",
            "Epoch 398/1000, Avg Loss: 0.447736\n",
            "Epoch 399/1000, Avg Loss: 0.447392\n",
            "Epoch 400/1000, Avg Loss: 0.461877\n",
            "Epoch 401/1000, Avg Loss: 0.447727\n",
            "Epoch 402/1000, Avg Loss: 0.463805\n",
            "Epoch 403/1000, Avg Loss: 0.453697\n",
            "Epoch 404/1000, Avg Loss: 0.451981\n",
            "Epoch 405/1000, Avg Loss: 0.446359\n",
            "Epoch 406/1000, Avg Loss: 0.456332\n",
            "Epoch 407/1000, Avg Loss: 0.454818\n",
            "Epoch 408/1000, Avg Loss: 0.454316\n",
            "Epoch 409/1000, Avg Loss: 0.451689\n",
            "Epoch 410/1000, Avg Loss: 0.437186\n",
            "Epoch 411/1000, Avg Loss: 0.454095\n",
            "Epoch 412/1000, Avg Loss: 0.448173\n",
            "Epoch 413/1000, Avg Loss: 0.444233\n",
            "Epoch 414/1000, Avg Loss: 0.452198\n",
            "Epoch 415/1000, Avg Loss: 0.449227\n",
            "Epoch 416/1000, Avg Loss: 0.438749\n",
            "Epoch 417/1000, Avg Loss: 0.449716\n",
            "Epoch 418/1000, Avg Loss: 0.453418\n",
            "Epoch 419/1000, Avg Loss: 0.459317\n",
            "Epoch 420/1000, Avg Loss: 0.449384\n",
            "Epoch 421/1000, Avg Loss: 0.451410\n",
            "Epoch 422/1000, Avg Loss: 0.439507\n",
            "Epoch 423/1000, Avg Loss: 0.443359\n",
            "Epoch 424/1000, Avg Loss: 0.457262\n",
            "Epoch 425/1000, Avg Loss: 0.442267\n",
            "Epoch 426/1000, Avg Loss: 0.461575\n",
            "Epoch 427/1000, Avg Loss: 0.448875\n",
            "Epoch 428/1000, Avg Loss: 0.454271\n",
            "Epoch 429/1000, Avg Loss: 0.456686\n",
            "Epoch 430/1000, Avg Loss: 0.442980\n",
            "Epoch 431/1000, Avg Loss: 0.464452\n",
            "Epoch 432/1000, Avg Loss: 0.455629\n",
            "Epoch 433/1000, Avg Loss: 0.465427\n",
            "Epoch 434/1000, Avg Loss: 0.446938\n",
            "Epoch 435/1000, Avg Loss: 0.454681\n",
            "Epoch 436/1000, Avg Loss: 0.448113\n",
            "Epoch 437/1000, Avg Loss: 0.450976\n",
            "Epoch 438/1000, Avg Loss: 0.453915\n",
            "Epoch 439/1000, Avg Loss: 0.448759\n",
            "Epoch 440/1000, Avg Loss: 0.451808\n",
            "Epoch 441/1000, Avg Loss: 0.448722\n",
            "Epoch 442/1000, Avg Loss: 0.439612\n",
            "Epoch 443/1000, Avg Loss: 0.438905\n",
            "Epoch 444/1000, Avg Loss: 0.442820\n",
            "Epoch 445/1000, Avg Loss: 0.441034\n",
            "Epoch 446/1000, Avg Loss: 0.452537\n",
            "Epoch 447/1000, Avg Loss: 0.443954\n",
            "Epoch 448/1000, Avg Loss: 0.449375\n",
            "Epoch 449/1000, Avg Loss: 0.453116\n",
            "Epoch 450/1000, Avg Loss: 0.453183\n",
            "Epoch 451/1000, Avg Loss: 0.452140\n",
            "Epoch 452/1000, Avg Loss: 0.444793\n",
            "Epoch 453/1000, Avg Loss: 0.441396\n",
            "Epoch 454/1000, Avg Loss: 0.460994\n",
            "Epoch 455/1000, Avg Loss: 0.451547\n",
            "Epoch 456/1000, Avg Loss: 0.443761\n",
            "Epoch 457/1000, Avg Loss: 0.444058\n",
            "Epoch 458/1000, Avg Loss: 0.458163\n",
            "Epoch 459/1000, Avg Loss: 0.440874\n",
            "Epoch 460/1000, Avg Loss: 0.436493\n",
            "Epoch 461/1000, Avg Loss: 0.434329\n",
            "Epoch 462/1000, Avg Loss: 0.434488\n",
            "Epoch 463/1000, Avg Loss: 0.448552\n",
            "Epoch 464/1000, Avg Loss: 0.440460\n",
            "Epoch 465/1000, Avg Loss: 0.436235\n",
            "Epoch 466/1000, Avg Loss: 0.441730\n",
            "Epoch 467/1000, Avg Loss: 0.436886\n",
            "Epoch 468/1000, Avg Loss: 0.448691\n",
            "Epoch 469/1000, Avg Loss: 0.443777\n",
            "Epoch 470/1000, Avg Loss: 0.450862\n",
            "Epoch 471/1000, Avg Loss: 0.447494\n",
            "Epoch 472/1000, Avg Loss: 0.447928\n",
            "Epoch 473/1000, Avg Loss: 0.440469\n",
            "Epoch 474/1000, Avg Loss: 0.434100\n",
            "Epoch 475/1000, Avg Loss: 0.440041\n",
            "Epoch 476/1000, Avg Loss: 0.448120\n",
            "Epoch 477/1000, Avg Loss: 0.441552\n",
            "Epoch 478/1000, Avg Loss: 0.449790\n",
            "Epoch 479/1000, Avg Loss: 0.451669\n",
            "Epoch 480/1000, Avg Loss: 0.430737\n",
            "Epoch 481/1000, Avg Loss: 0.441785\n",
            "Epoch 482/1000, Avg Loss: 0.446018\n",
            "Epoch 483/1000, Avg Loss: 0.431319\n",
            "Epoch 484/1000, Avg Loss: 0.441501\n",
            "Epoch 485/1000, Avg Loss: 0.452552\n",
            "Epoch 486/1000, Avg Loss: 0.442727\n",
            "Epoch 487/1000, Avg Loss: 0.432760\n",
            "Epoch 488/1000, Avg Loss: 0.438615\n",
            "Epoch 489/1000, Avg Loss: 0.446160\n",
            "Epoch 490/1000, Avg Loss: 0.443700\n",
            "Epoch 491/1000, Avg Loss: 0.448353\n",
            "Epoch 492/1000, Avg Loss: 0.437540\n",
            "Epoch 493/1000, Avg Loss: 0.452260\n",
            "Epoch 494/1000, Avg Loss: 0.434383\n",
            "Epoch 495/1000, Avg Loss: 0.445240\n",
            "Epoch 496/1000, Avg Loss: 0.445352\n",
            "Epoch 497/1000, Avg Loss: 0.443999\n",
            "Epoch 498/1000, Avg Loss: 0.440638\n",
            "Epoch 499/1000, Avg Loss: 0.447156\n",
            "Epoch 500/1000, Avg Loss: 0.454413\n",
            "Epoch 501/1000, Avg Loss: 0.446566\n",
            "Epoch 502/1000, Avg Loss: 0.444632\n",
            "Epoch 503/1000, Avg Loss: 0.460373\n",
            "Epoch 504/1000, Avg Loss: 0.444397\n",
            "Epoch 505/1000, Avg Loss: 0.449875\n",
            "Epoch 506/1000, Avg Loss: 0.431432\n",
            "Epoch 507/1000, Avg Loss: 0.444922\n",
            "Epoch 508/1000, Avg Loss: 0.447334\n",
            "Epoch 509/1000, Avg Loss: 0.425071\n",
            "Epoch 510/1000, Avg Loss: 0.439298\n",
            "Epoch 511/1000, Avg Loss: 0.435467\n",
            "Epoch 512/1000, Avg Loss: 0.441613\n",
            "Epoch 513/1000, Avg Loss: 0.432875\n",
            "Epoch 514/1000, Avg Loss: 0.441487\n",
            "Epoch 515/1000, Avg Loss: 0.443770\n",
            "Epoch 516/1000, Avg Loss: 0.452173\n",
            "Epoch 517/1000, Avg Loss: 0.446455\n",
            "Epoch 518/1000, Avg Loss: 0.440463\n",
            "Epoch 519/1000, Avg Loss: 0.432069\n",
            "Epoch 520/1000, Avg Loss: 0.438261\n",
            "Epoch 521/1000, Avg Loss: 0.441877\n",
            "Epoch 522/1000, Avg Loss: 0.433143\n",
            "Epoch 523/1000, Avg Loss: 0.451303\n",
            "Epoch 524/1000, Avg Loss: 0.432080\n",
            "Epoch 525/1000, Avg Loss: 0.437227\n",
            "Epoch 526/1000, Avg Loss: 0.446130\n",
            "Epoch 527/1000, Avg Loss: 0.439270\n",
            "Epoch 528/1000, Avg Loss: 0.452884\n",
            "Epoch 529/1000, Avg Loss: 0.426602\n",
            "Epoch 530/1000, Avg Loss: 0.438326\n",
            "Epoch 531/1000, Avg Loss: 0.439058\n",
            "Epoch 532/1000, Avg Loss: 0.436384\n",
            "Epoch 533/1000, Avg Loss: 0.430977\n",
            "Epoch 534/1000, Avg Loss: 0.427064\n",
            "Epoch 535/1000, Avg Loss: 0.438273\n",
            "Epoch 536/1000, Avg Loss: 0.437463\n",
            "Epoch 537/1000, Avg Loss: 0.438830\n",
            "Epoch 538/1000, Avg Loss: 0.427944\n",
            "Epoch 539/1000, Avg Loss: 0.433782\n",
            "Epoch 540/1000, Avg Loss: 0.446198\n",
            "Epoch 541/1000, Avg Loss: 0.439732\n",
            "Epoch 542/1000, Avg Loss: 0.436529\n",
            "Epoch 543/1000, Avg Loss: 0.440346\n",
            "Epoch 544/1000, Avg Loss: 0.448248\n",
            "Epoch 545/1000, Avg Loss: 0.450243\n",
            "Epoch 546/1000, Avg Loss: 0.426088\n",
            "Epoch 547/1000, Avg Loss: 0.423141\n",
            "Epoch 548/1000, Avg Loss: 0.438470\n",
            "Epoch 549/1000, Avg Loss: 0.452727\n",
            "Epoch 550/1000, Avg Loss: 0.430005\n",
            "Epoch 551/1000, Avg Loss: 0.441754\n",
            "Epoch 552/1000, Avg Loss: 0.444866\n",
            "Epoch 553/1000, Avg Loss: 0.440620\n",
            "Epoch 554/1000, Avg Loss: 0.459038\n",
            "Epoch 555/1000, Avg Loss: 0.431088\n",
            "Epoch 556/1000, Avg Loss: 0.439744\n",
            "Epoch 557/1000, Avg Loss: 0.434784\n",
            "Epoch 558/1000, Avg Loss: 0.435741\n",
            "Epoch 559/1000, Avg Loss: 0.428497\n",
            "Epoch 560/1000, Avg Loss: 0.438845\n",
            "Epoch 561/1000, Avg Loss: 0.444469\n",
            "Epoch 562/1000, Avg Loss: 0.438877\n",
            "Epoch 563/1000, Avg Loss: 0.435917\n",
            "Epoch 564/1000, Avg Loss: 0.435776\n",
            "Epoch 565/1000, Avg Loss: 0.442193\n",
            "Epoch 566/1000, Avg Loss: 0.433030\n",
            "Epoch 567/1000, Avg Loss: 0.431729\n",
            "Epoch 568/1000, Avg Loss: 0.435081\n",
            "Epoch 569/1000, Avg Loss: 0.435785\n",
            "Epoch 570/1000, Avg Loss: 0.426273\n",
            "Epoch 571/1000, Avg Loss: 0.444019\n",
            "Epoch 572/1000, Avg Loss: 0.441907\n",
            "Epoch 573/1000, Avg Loss: 0.432317\n",
            "Epoch 574/1000, Avg Loss: 0.436528\n",
            "Epoch 575/1000, Avg Loss: 0.420309\n",
            "Epoch 576/1000, Avg Loss: 0.448801\n",
            "Epoch 577/1000, Avg Loss: 0.436838\n",
            "Epoch 578/1000, Avg Loss: 0.435104\n",
            "Epoch 579/1000, Avg Loss: 0.441295\n",
            "Epoch 580/1000, Avg Loss: 0.433442\n",
            "Epoch 581/1000, Avg Loss: 0.433515\n",
            "Epoch 582/1000, Avg Loss: 0.443152\n",
            "Epoch 583/1000, Avg Loss: 0.444878\n",
            "Epoch 584/1000, Avg Loss: 0.425812\n",
            "Epoch 585/1000, Avg Loss: 0.434118\n",
            "Epoch 586/1000, Avg Loss: 0.444229\n",
            "Epoch 587/1000, Avg Loss: 0.434925\n",
            "Epoch 588/1000, Avg Loss: 0.429265\n",
            "Epoch 589/1000, Avg Loss: 0.427617\n",
            "Epoch 590/1000, Avg Loss: 0.438945\n",
            "Epoch 591/1000, Avg Loss: 0.427050\n",
            "Epoch 592/1000, Avg Loss: 0.433288\n",
            "Epoch 593/1000, Avg Loss: 0.449964\n",
            "Epoch 594/1000, Avg Loss: 0.425348\n",
            "Epoch 595/1000, Avg Loss: 0.437972\n",
            "Epoch 596/1000, Avg Loss: 0.442138\n",
            "Epoch 597/1000, Avg Loss: 0.434922\n",
            "Epoch 598/1000, Avg Loss: 0.435802\n",
            "Epoch 599/1000, Avg Loss: 0.433066\n",
            "Epoch 600/1000, Avg Loss: 0.441392\n",
            "Epoch 601/1000, Avg Loss: 0.448659\n",
            "Epoch 602/1000, Avg Loss: 0.435197\n",
            "Epoch 603/1000, Avg Loss: 0.450699\n",
            "Epoch 604/1000, Avg Loss: 0.440981\n",
            "Epoch 605/1000, Avg Loss: 0.433821\n",
            "Epoch 606/1000, Avg Loss: 0.431291\n",
            "Epoch 607/1000, Avg Loss: 0.431879\n",
            "Epoch 608/1000, Avg Loss: 0.439140\n",
            "Epoch 609/1000, Avg Loss: 0.431998\n",
            "Epoch 610/1000, Avg Loss: 0.437738\n",
            "Epoch 611/1000, Avg Loss: 0.441178\n",
            "Epoch 612/1000, Avg Loss: 0.436776\n",
            "Epoch 613/1000, Avg Loss: 0.440071\n",
            "Epoch 614/1000, Avg Loss: 0.435480\n",
            "Epoch 615/1000, Avg Loss: 0.447857\n",
            "Epoch 616/1000, Avg Loss: 0.438055\n",
            "Epoch 617/1000, Avg Loss: 0.435666\n",
            "Epoch 618/1000, Avg Loss: 0.437087\n",
            "Epoch 619/1000, Avg Loss: 0.442544\n",
            "Epoch 620/1000, Avg Loss: 0.424140\n",
            "Epoch 621/1000, Avg Loss: 0.432654\n",
            "Epoch 622/1000, Avg Loss: 0.424684\n",
            "Epoch 623/1000, Avg Loss: 0.420171\n",
            "Epoch 624/1000, Avg Loss: 0.450801\n",
            "Epoch 625/1000, Avg Loss: 0.440972\n",
            "Epoch 626/1000, Avg Loss: 0.435974\n",
            "Epoch 627/1000, Avg Loss: 0.440116\n",
            "Epoch 628/1000, Avg Loss: 0.427329\n",
            "Epoch 629/1000, Avg Loss: 0.436952\n",
            "Epoch 630/1000, Avg Loss: 0.435361\n",
            "Epoch 631/1000, Avg Loss: 0.439428\n",
            "Epoch 632/1000, Avg Loss: 0.424922\n",
            "Epoch 633/1000, Avg Loss: 0.429700\n",
            "Epoch 634/1000, Avg Loss: 0.432983\n",
            "Epoch 635/1000, Avg Loss: 0.437550\n",
            "Epoch 636/1000, Avg Loss: 0.440265\n",
            "Epoch 637/1000, Avg Loss: 0.437663\n",
            "Epoch 638/1000, Avg Loss: 0.443450\n",
            "Epoch 639/1000, Avg Loss: 0.452205\n",
            "Epoch 640/1000, Avg Loss: 0.433691\n",
            "Epoch 641/1000, Avg Loss: 0.435654\n",
            "Epoch 642/1000, Avg Loss: 0.433826\n",
            "Epoch 643/1000, Avg Loss: 0.433886\n",
            "Epoch 644/1000, Avg Loss: 0.436686\n",
            "Epoch 645/1000, Avg Loss: 0.431684\n",
            "Epoch 646/1000, Avg Loss: 0.427969\n",
            "Epoch 647/1000, Avg Loss: 0.433904\n",
            "Epoch 648/1000, Avg Loss: 0.437334\n",
            "Epoch 649/1000, Avg Loss: 0.423052\n",
            "Epoch 650/1000, Avg Loss: 0.427422\n",
            "Epoch 651/1000, Avg Loss: 0.436380\n",
            "Epoch 652/1000, Avg Loss: 0.432369\n",
            "Epoch 653/1000, Avg Loss: 0.440461\n",
            "Epoch 654/1000, Avg Loss: 0.434618\n",
            "Epoch 655/1000, Avg Loss: 0.434934\n",
            "Epoch 656/1000, Avg Loss: 0.427334\n",
            "Epoch 657/1000, Avg Loss: 0.428207\n",
            "Epoch 658/1000, Avg Loss: 0.432867\n",
            "Epoch 659/1000, Avg Loss: 0.432091\n",
            "Epoch 660/1000, Avg Loss: 0.431032\n",
            "Epoch 661/1000, Avg Loss: 0.432584\n",
            "Epoch 662/1000, Avg Loss: 0.432584\n",
            "Epoch 663/1000, Avg Loss: 0.448050\n",
            "Epoch 664/1000, Avg Loss: 0.426433\n",
            "Epoch 665/1000, Avg Loss: 0.424956\n",
            "Epoch 666/1000, Avg Loss: 0.442737\n",
            "Epoch 667/1000, Avg Loss: 0.444322\n",
            "Epoch 668/1000, Avg Loss: 0.432475\n",
            "Epoch 669/1000, Avg Loss: 0.431541\n",
            "Epoch 670/1000, Avg Loss: 0.441350\n",
            "Epoch 671/1000, Avg Loss: 0.422669\n",
            "Epoch 672/1000, Avg Loss: 0.429767\n",
            "Epoch 673/1000, Avg Loss: 0.426223\n",
            "Epoch 674/1000, Avg Loss: 0.428353\n",
            "Epoch 675/1000, Avg Loss: 0.434955\n",
            "Epoch 676/1000, Avg Loss: 0.430971\n",
            "Epoch 677/1000, Avg Loss: 0.442464\n",
            "Epoch 678/1000, Avg Loss: 0.424197\n",
            "Epoch 679/1000, Avg Loss: 0.430712\n",
            "Epoch 680/1000, Avg Loss: 0.423127\n",
            "Epoch 681/1000, Avg Loss: 0.436163\n",
            "Epoch 682/1000, Avg Loss: 0.433388\n",
            "Epoch 683/1000, Avg Loss: 0.435067\n",
            "Epoch 684/1000, Avg Loss: 0.423810\n",
            "Epoch 685/1000, Avg Loss: 0.436379\n",
            "Epoch 686/1000, Avg Loss: 0.420671\n",
            "Epoch 687/1000, Avg Loss: 0.429955\n",
            "Epoch 688/1000, Avg Loss: 0.437838\n",
            "Epoch 689/1000, Avg Loss: 0.433674\n",
            "Epoch 690/1000, Avg Loss: 0.438586\n",
            "Epoch 691/1000, Avg Loss: 0.424674\n",
            "Epoch 692/1000, Avg Loss: 0.431854\n",
            "Epoch 693/1000, Avg Loss: 0.431680\n",
            "Epoch 694/1000, Avg Loss: 0.447943\n",
            "Epoch 695/1000, Avg Loss: 0.436987\n",
            "Epoch 696/1000, Avg Loss: 0.424744\n",
            "Epoch 697/1000, Avg Loss: 0.443250\n",
            "Epoch 698/1000, Avg Loss: 0.429977\n",
            "Epoch 699/1000, Avg Loss: 0.425204\n",
            "Epoch 700/1000, Avg Loss: 0.428116\n",
            "Epoch 701/1000, Avg Loss: 0.433379\n",
            "Epoch 702/1000, Avg Loss: 0.416441\n",
            "Epoch 703/1000, Avg Loss: 0.436450\n",
            "Epoch 704/1000, Avg Loss: 0.430712\n",
            "Epoch 705/1000, Avg Loss: 0.430627\n",
            "Epoch 706/1000, Avg Loss: 0.423578\n",
            "Epoch 707/1000, Avg Loss: 0.425184\n",
            "Epoch 708/1000, Avg Loss: 0.436442\n",
            "Epoch 709/1000, Avg Loss: 0.432721\n",
            "Epoch 710/1000, Avg Loss: 0.438754\n",
            "Epoch 711/1000, Avg Loss: 0.419496\n",
            "Epoch 712/1000, Avg Loss: 0.440664\n",
            "Epoch 713/1000, Avg Loss: 0.426737\n",
            "Epoch 714/1000, Avg Loss: 0.429495\n",
            "Epoch 715/1000, Avg Loss: 0.438822\n",
            "Epoch 716/1000, Avg Loss: 0.427001\n",
            "Epoch 717/1000, Avg Loss: 0.425394\n",
            "Epoch 718/1000, Avg Loss: 0.425487\n",
            "Epoch 719/1000, Avg Loss: 0.434615\n",
            "Epoch 720/1000, Avg Loss: 0.428340\n",
            "Epoch 721/1000, Avg Loss: 0.424797\n",
            "Epoch 722/1000, Avg Loss: 0.427044\n",
            "Epoch 723/1000, Avg Loss: 0.421771\n",
            "Epoch 724/1000, Avg Loss: 0.431944\n",
            "Epoch 725/1000, Avg Loss: 0.421293\n",
            "Epoch 726/1000, Avg Loss: 0.431767\n",
            "Epoch 727/1000, Avg Loss: 0.426849\n",
            "Epoch 728/1000, Avg Loss: 0.425690\n",
            "Epoch 729/1000, Avg Loss: 0.437775\n",
            "Epoch 730/1000, Avg Loss: 0.424901\n",
            "Epoch 731/1000, Avg Loss: 0.410559\n",
            "Epoch 732/1000, Avg Loss: 0.431049\n",
            "Epoch 733/1000, Avg Loss: 0.418129\n",
            "Epoch 734/1000, Avg Loss: 0.428371\n",
            "Epoch 735/1000, Avg Loss: 0.428078\n",
            "Epoch 736/1000, Avg Loss: 0.428359\n",
            "Epoch 737/1000, Avg Loss: 0.429836\n",
            "Epoch 738/1000, Avg Loss: 0.414533\n",
            "Epoch 739/1000, Avg Loss: 0.422797\n",
            "Epoch 740/1000, Avg Loss: 0.421307\n",
            "Epoch 741/1000, Avg Loss: 0.434205\n",
            "Epoch 742/1000, Avg Loss: 0.411132\n",
            "Epoch 743/1000, Avg Loss: 0.426070\n",
            "Epoch 744/1000, Avg Loss: 0.420973\n",
            "Epoch 745/1000, Avg Loss: 0.416173\n",
            "Epoch 746/1000, Avg Loss: 0.439842\n",
            "Epoch 747/1000, Avg Loss: 0.433178\n",
            "Epoch 748/1000, Avg Loss: 0.426120\n",
            "Epoch 749/1000, Avg Loss: 0.424560\n",
            "Epoch 750/1000, Avg Loss: 0.428725\n",
            "Epoch 751/1000, Avg Loss: 0.434234\n",
            "Epoch 752/1000, Avg Loss: 0.429697\n",
            "Epoch 753/1000, Avg Loss: 0.431668\n",
            "Epoch 754/1000, Avg Loss: 0.430820\n",
            "Epoch 755/1000, Avg Loss: 0.440040\n",
            "Epoch 756/1000, Avg Loss: 0.443224\n",
            "Epoch 757/1000, Avg Loss: 0.423031\n",
            "Epoch 758/1000, Avg Loss: 0.417360\n",
            "Epoch 759/1000, Avg Loss: 0.436529\n",
            "Epoch 760/1000, Avg Loss: 0.414611\n",
            "Epoch 761/1000, Avg Loss: 0.436400\n",
            "Epoch 762/1000, Avg Loss: 0.420642\n",
            "Epoch 763/1000, Avg Loss: 0.422025\n",
            "Epoch 764/1000, Avg Loss: 0.415205\n",
            "Epoch 765/1000, Avg Loss: 0.418378\n",
            "Epoch 766/1000, Avg Loss: 0.429575\n",
            "Epoch 767/1000, Avg Loss: 0.412794\n",
            "Epoch 768/1000, Avg Loss: 0.418738\n",
            "Epoch 769/1000, Avg Loss: 0.419202\n",
            "Epoch 770/1000, Avg Loss: 0.419803\n",
            "Epoch 771/1000, Avg Loss: 0.433993\n",
            "Epoch 772/1000, Avg Loss: 0.429572\n",
            "Epoch 773/1000, Avg Loss: 0.440207\n",
            "Epoch 774/1000, Avg Loss: 0.419639\n",
            "Epoch 775/1000, Avg Loss: 0.417404\n",
            "Epoch 776/1000, Avg Loss: 0.424356\n",
            "Epoch 777/1000, Avg Loss: 0.424694\n",
            "Epoch 778/1000, Avg Loss: 0.428277\n",
            "Epoch 779/1000, Avg Loss: 0.424392\n",
            "Epoch 780/1000, Avg Loss: 0.434177\n",
            "Epoch 781/1000, Avg Loss: 0.427203\n",
            "Epoch 782/1000, Avg Loss: 0.421594\n",
            "Epoch 783/1000, Avg Loss: 0.425537\n",
            "Epoch 784/1000, Avg Loss: 0.428073\n",
            "Epoch 785/1000, Avg Loss: 0.412604\n",
            "Epoch 786/1000, Avg Loss: 0.421276\n",
            "Epoch 787/1000, Avg Loss: 0.409813\n",
            "Epoch 788/1000, Avg Loss: 0.428165\n",
            "Epoch 789/1000, Avg Loss: 0.424607\n",
            "Epoch 790/1000, Avg Loss: 0.412807\n",
            "Epoch 791/1000, Avg Loss: 0.426208\n",
            "Epoch 792/1000, Avg Loss: 0.424729\n",
            "Epoch 793/1000, Avg Loss: 0.425314\n",
            "Epoch 794/1000, Avg Loss: 0.405078\n",
            "Epoch 795/1000, Avg Loss: 0.421454\n",
            "Epoch 796/1000, Avg Loss: 0.427468\n",
            "Epoch 797/1000, Avg Loss: 0.429813\n",
            "Epoch 798/1000, Avg Loss: 0.420269\n",
            "Epoch 799/1000, Avg Loss: 0.416626\n",
            "Epoch 800/1000, Avg Loss: 0.416930\n",
            "Epoch 801/1000, Avg Loss: 0.441920\n",
            "Epoch 802/1000, Avg Loss: 0.430181\n",
            "Epoch 803/1000, Avg Loss: 0.426007\n",
            "Epoch 804/1000, Avg Loss: 0.424727\n",
            "Epoch 805/1000, Avg Loss: 0.437745\n",
            "Epoch 806/1000, Avg Loss: 0.418512\n",
            "Epoch 807/1000, Avg Loss: 0.424660\n",
            "Epoch 808/1000, Avg Loss: 0.419575\n",
            "Epoch 809/1000, Avg Loss: 0.429410\n",
            "Epoch 810/1000, Avg Loss: 0.428829\n",
            "Epoch 811/1000, Avg Loss: 0.434397\n",
            "Epoch 812/1000, Avg Loss: 0.407949\n",
            "Epoch 813/1000, Avg Loss: 0.430464\n",
            "Epoch 814/1000, Avg Loss: 0.422145\n",
            "Epoch 815/1000, Avg Loss: 0.415202\n",
            "Epoch 816/1000, Avg Loss: 0.419112\n",
            "Epoch 817/1000, Avg Loss: 0.419813\n",
            "Epoch 818/1000, Avg Loss: 0.429081\n",
            "Epoch 819/1000, Avg Loss: 0.425057\n",
            "Epoch 820/1000, Avg Loss: 0.417751\n",
            "Epoch 821/1000, Avg Loss: 0.427989\n",
            "Epoch 822/1000, Avg Loss: 0.434072\n",
            "Epoch 823/1000, Avg Loss: 0.417562\n",
            "Epoch 824/1000, Avg Loss: 0.414166\n",
            "Epoch 825/1000, Avg Loss: 0.408625\n",
            "Epoch 826/1000, Avg Loss: 0.428387\n",
            "Epoch 827/1000, Avg Loss: 0.426645\n",
            "Epoch 828/1000, Avg Loss: 0.421466\n",
            "Epoch 829/1000, Avg Loss: 0.428304\n",
            "Epoch 830/1000, Avg Loss: 0.424176\n",
            "Epoch 831/1000, Avg Loss: 0.409754\n",
            "Epoch 832/1000, Avg Loss: 0.419891\n",
            "Epoch 833/1000, Avg Loss: 0.432203\n",
            "Epoch 834/1000, Avg Loss: 0.418501\n",
            "Epoch 835/1000, Avg Loss: 0.436795\n",
            "Epoch 836/1000, Avg Loss: 0.431300\n",
            "Epoch 837/1000, Avg Loss: 0.420605\n",
            "Epoch 838/1000, Avg Loss: 0.428377\n",
            "Epoch 839/1000, Avg Loss: 0.420501\n",
            "Epoch 840/1000, Avg Loss: 0.423078\n",
            "Epoch 841/1000, Avg Loss: 0.426958\n",
            "Epoch 842/1000, Avg Loss: 0.426956\n",
            "Epoch 843/1000, Avg Loss: 0.423093\n",
            "Epoch 844/1000, Avg Loss: 0.424039\n",
            "Epoch 845/1000, Avg Loss: 0.419903\n",
            "Epoch 846/1000, Avg Loss: 0.427570\n",
            "Epoch 847/1000, Avg Loss: 0.425348\n",
            "Epoch 848/1000, Avg Loss: 0.414647\n",
            "Epoch 849/1000, Avg Loss: 0.421854\n",
            "Epoch 850/1000, Avg Loss: 0.414215\n",
            "Epoch 851/1000, Avg Loss: 0.432019\n",
            "Epoch 852/1000, Avg Loss: 0.410136\n",
            "Epoch 853/1000, Avg Loss: 0.409929\n",
            "Epoch 854/1000, Avg Loss: 0.424808\n",
            "Epoch 855/1000, Avg Loss: 0.421250\n",
            "Epoch 856/1000, Avg Loss: 0.426026\n",
            "Epoch 857/1000, Avg Loss: 0.426537\n",
            "Epoch 858/1000, Avg Loss: 0.418990\n",
            "Epoch 859/1000, Avg Loss: 0.428008\n",
            "Epoch 860/1000, Avg Loss: 0.409407\n",
            "Epoch 861/1000, Avg Loss: 0.410444\n",
            "Epoch 862/1000, Avg Loss: 0.420708\n",
            "Epoch 863/1000, Avg Loss: 0.427774\n",
            "Epoch 864/1000, Avg Loss: 0.430783\n",
            "Epoch 865/1000, Avg Loss: 0.424055\n",
            "Epoch 866/1000, Avg Loss: 0.416725\n",
            "Epoch 867/1000, Avg Loss: 0.434047\n",
            "Epoch 868/1000, Avg Loss: 0.414882\n",
            "Epoch 869/1000, Avg Loss: 0.413308\n",
            "Epoch 870/1000, Avg Loss: 0.418621\n",
            "Epoch 871/1000, Avg Loss: 0.422791\n",
            "Epoch 872/1000, Avg Loss: 0.420490\n",
            "Epoch 873/1000, Avg Loss: 0.427481\n",
            "Epoch 874/1000, Avg Loss: 0.431644\n",
            "Epoch 875/1000, Avg Loss: 0.433561\n",
            "Epoch 876/1000, Avg Loss: 0.401047\n",
            "Epoch 877/1000, Avg Loss: 0.425322\n",
            "Epoch 878/1000, Avg Loss: 0.412348\n",
            "Epoch 879/1000, Avg Loss: 0.432507\n",
            "Epoch 880/1000, Avg Loss: 0.417611\n",
            "Epoch 881/1000, Avg Loss: 0.411795\n",
            "Epoch 882/1000, Avg Loss: 0.414055\n",
            "Epoch 883/1000, Avg Loss: 0.408376\n",
            "Epoch 884/1000, Avg Loss: 0.414552\n",
            "Epoch 885/1000, Avg Loss: 0.417263\n",
            "Epoch 886/1000, Avg Loss: 0.419581\n",
            "Epoch 887/1000, Avg Loss: 0.407803\n",
            "Epoch 888/1000, Avg Loss: 0.418697\n",
            "Epoch 889/1000, Avg Loss: 0.432685\n",
            "Epoch 890/1000, Avg Loss: 0.408441\n",
            "Epoch 891/1000, Avg Loss: 0.428158\n",
            "Epoch 892/1000, Avg Loss: 0.413309\n",
            "Epoch 893/1000, Avg Loss: 0.419094\n",
            "Epoch 894/1000, Avg Loss: 0.416868\n",
            "Epoch 895/1000, Avg Loss: 0.414327\n",
            "Epoch 896/1000, Avg Loss: 0.419055\n",
            "Epoch 897/1000, Avg Loss: 0.418510\n",
            "Epoch 898/1000, Avg Loss: 0.425895\n",
            "Epoch 899/1000, Avg Loss: 0.419541\n",
            "Epoch 900/1000, Avg Loss: 0.422153\n",
            "Epoch 901/1000, Avg Loss: 0.415662\n",
            "Epoch 902/1000, Avg Loss: 0.416067\n",
            "Epoch 903/1000, Avg Loss: 0.408343\n",
            "Epoch 904/1000, Avg Loss: 0.428358\n",
            "Epoch 905/1000, Avg Loss: 0.417508\n",
            "Epoch 906/1000, Avg Loss: 0.412882\n",
            "Epoch 907/1000, Avg Loss: 0.421182\n",
            "Epoch 908/1000, Avg Loss: 0.423054\n",
            "Epoch 909/1000, Avg Loss: 0.420335\n",
            "Epoch 910/1000, Avg Loss: 0.410309\n",
            "Epoch 911/1000, Avg Loss: 0.416648\n",
            "Epoch 912/1000, Avg Loss: 0.419963\n",
            "Epoch 913/1000, Avg Loss: 0.419420\n",
            "Epoch 914/1000, Avg Loss: 0.416050\n",
            "Epoch 915/1000, Avg Loss: 0.419567\n",
            "Epoch 916/1000, Avg Loss: 0.413257\n",
            "Epoch 917/1000, Avg Loss: 0.425295\n",
            "Epoch 918/1000, Avg Loss: 0.417639\n",
            "Epoch 919/1000, Avg Loss: 0.410867\n",
            "Epoch 920/1000, Avg Loss: 0.411448\n",
            "Epoch 921/1000, Avg Loss: 0.418402\n",
            "Epoch 922/1000, Avg Loss: 0.416897\n",
            "Epoch 923/1000, Avg Loss: 0.416105\n",
            "Epoch 924/1000, Avg Loss: 0.423081\n",
            "Epoch 925/1000, Avg Loss: 0.410908\n",
            "Epoch 926/1000, Avg Loss: 0.409298\n",
            "Epoch 927/1000, Avg Loss: 0.423750\n",
            "Epoch 928/1000, Avg Loss: 0.425910\n",
            "Epoch 929/1000, Avg Loss: 0.417246\n",
            "Epoch 930/1000, Avg Loss: 0.415252\n",
            "Epoch 931/1000, Avg Loss: 0.416907\n",
            "Epoch 932/1000, Avg Loss: 0.410564\n",
            "Epoch 933/1000, Avg Loss: 0.418999\n",
            "Epoch 934/1000, Avg Loss: 0.408154\n",
            "Epoch 935/1000, Avg Loss: 0.416083\n",
            "Epoch 936/1000, Avg Loss: 0.412543\n",
            "Epoch 937/1000, Avg Loss: 0.417767\n",
            "Epoch 938/1000, Avg Loss: 0.414184\n",
            "Epoch 939/1000, Avg Loss: 0.422024\n",
            "Epoch 940/1000, Avg Loss: 0.417607\n",
            "Epoch 941/1000, Avg Loss: 0.418959\n",
            "Epoch 942/1000, Avg Loss: 0.426722\n",
            "Epoch 943/1000, Avg Loss: 0.418351\n",
            "Epoch 944/1000, Avg Loss: 0.419450\n",
            "Epoch 945/1000, Avg Loss: 0.407329\n",
            "Epoch 946/1000, Avg Loss: 0.427575\n",
            "Epoch 947/1000, Avg Loss: 0.415282\n",
            "Epoch 948/1000, Avg Loss: 0.417659\n",
            "Epoch 949/1000, Avg Loss: 0.427835\n",
            "Epoch 950/1000, Avg Loss: 0.427976\n",
            "Epoch 951/1000, Avg Loss: 0.402524\n",
            "Epoch 952/1000, Avg Loss: 0.412662\n",
            "Epoch 953/1000, Avg Loss: 0.417486\n",
            "Epoch 954/1000, Avg Loss: 0.429549\n",
            "Epoch 955/1000, Avg Loss: 0.423907\n",
            "Epoch 956/1000, Avg Loss: 0.420730\n",
            "Epoch 957/1000, Avg Loss: 0.401834\n",
            "Epoch 958/1000, Avg Loss: 0.402744\n",
            "Epoch 959/1000, Avg Loss: 0.413026\n",
            "Epoch 960/1000, Avg Loss: 0.414863\n",
            "Epoch 961/1000, Avg Loss: 0.422390\n",
            "Epoch 962/1000, Avg Loss: 0.409194\n",
            "Epoch 963/1000, Avg Loss: 0.405933\n",
            "Epoch 964/1000, Avg Loss: 0.415253\n",
            "Epoch 965/1000, Avg Loss: 0.420749\n",
            "Epoch 966/1000, Avg Loss: 0.419427\n",
            "Epoch 967/1000, Avg Loss: 0.417216\n",
            "Epoch 968/1000, Avg Loss: 0.423887\n",
            "Epoch 969/1000, Avg Loss: 0.409537\n",
            "Epoch 970/1000, Avg Loss: 0.401463\n",
            "Epoch 971/1000, Avg Loss: 0.420714\n",
            "Epoch 972/1000, Avg Loss: 0.435451\n",
            "Epoch 973/1000, Avg Loss: 0.412325\n",
            "Epoch 974/1000, Avg Loss: 0.418352\n",
            "Epoch 975/1000, Avg Loss: 0.408290\n",
            "Epoch 976/1000, Avg Loss: 0.410033\n",
            "Epoch 977/1000, Avg Loss: 0.412368\n",
            "Epoch 978/1000, Avg Loss: 0.411596\n",
            "Epoch 979/1000, Avg Loss: 0.422280\n",
            "Epoch 980/1000, Avg Loss: 0.410881\n",
            "Epoch 981/1000, Avg Loss: 0.412550\n",
            "Epoch 982/1000, Avg Loss: 0.412857\n",
            "Epoch 983/1000, Avg Loss: 0.420735\n",
            "Epoch 984/1000, Avg Loss: 0.403274\n",
            "Epoch 985/1000, Avg Loss: 0.410793\n",
            "Epoch 986/1000, Avg Loss: 0.405169\n",
            "Epoch 987/1000, Avg Loss: 0.422214\n",
            "Epoch 988/1000, Avg Loss: 0.399168\n",
            "Epoch 989/1000, Avg Loss: 0.419160\n",
            "Epoch 990/1000, Avg Loss: 0.409320\n",
            "Epoch 991/1000, Avg Loss: 0.405621\n",
            "Epoch 992/1000, Avg Loss: 0.421027\n",
            "Epoch 993/1000, Avg Loss: 0.408577\n",
            "Epoch 994/1000, Avg Loss: 0.416096\n",
            "Epoch 995/1000, Avg Loss: 0.402571\n",
            "Epoch 996/1000, Avg Loss: 0.422515\n",
            "Epoch 997/1000, Avg Loss: 0.413879\n",
            "Epoch 998/1000, Avg Loss: 0.415387\n",
            "Epoch 999/1000, Avg Loss: 0.409553\n",
            "Epoch 1000/1000, Avg Loss: 0.415954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5: OOD Testing"
      ],
      "metadata": {
        "id": "CHSikfx6cKUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural.eval()\n",
        "\n",
        "test_loss = 0.0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, label in test_loader: #Iterate through the test_loader. Think of test_loader as a list.\n",
        "        output = neural(data).squeeze(-1)              #Returns output\n",
        "        batch_loss = criterion(output, label.float())   #Gets loss for each value\n",
        "        test_loss += batch_loss.item() * data.size(0). #Converts tensor to a float, so average *datasize is the total amount\n",
        "\n",
        "        # predictions: threshold directly\n",
        "        pred = (output > ).long()                                #Output should be greater than what?\n",
        "        all_preds.extend(pred.cpu().tolist())                    #Since you are doing it in batches how cna you use extensions here?\n",
        "        all_labels.extend(label.cpu().tolist())                 #Since you are doing it in batches how cna you use extensions here?\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])\n",
        "ConfusionMatrixDisplay(cm, display_labels=[\"Class 0\", \"Class 1\"]).plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# optional accuracy\n",
        "acc = (cm.trace() / cm.sum()) * 100\n",
        "print(f\"Accuracy: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "prN9P6bpvl2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now find do a hyperparameter grid search. Hopefully you found some hyperparameters you were trying to optimize for"
      ],
      "metadata": {
        "id": "fJkitHDZyZr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH = 256\n",
        "EPOCHS = 5                       # keep small per size; increase if you like\n",
        "LR = 1e-3\n",
        "SIZES = [1_000, 5_000, 10_000, 50_000]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for size in SIZES:\n",
        "    print(f\"\\n=== dataset_size = {size} ===\")\n",
        "    # datasets & loaders, I want all features in this case.\n",
        "    train_ds =\n",
        "    test_ds  =\n",
        "\n",
        "    #I want shuffle to be False\n",
        "    train_loader =\n",
        "    test_loader  =\n",
        "\n",
        "    # model / opt / loss (binary head with BCELoss)\n",
        "\n",
        "    #Instantiate model\n",
        "\n",
        "    #Set optimizer\n",
        "\n",
        "    #Set criterion again\n",
        "\n",
        "    # ----- train -----\n",
        "    neural.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        #Iterate through the train_loader\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = neural(data).squeeze(-1)        # probs in [0,1]\n",
        "            loss = criterion(out, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # ----- eval -----\n",
        "    neural.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data, label in test_loader:\n",
        "\n",
        "            #Part of evaluation is test loss\n",
        "            out = neural(data).squeeze(-1)        # Outputs Probability\n",
        "            test_loss += criterion(out, label.float()).item() * data.size(0) #Calculates loss for each\n",
        "            total += label.size(0)                                          #Number of data points ran\n",
        "\n",
        "\n",
        "            pred = ()                    # bool\n",
        "            all_probs.extend()\n",
        "            all_labels.extend()\n",
        "\n",
        "    avg_loss = test_loss / total\n",
        "    acc =                                     #Implement\n",
        "    cm = confusion_matrix(all_labels, np.array(all_probs) > 0.5, labels=[0,1])\n",
        "\n",
        "    results[size] = dict(loss=avg_loss, accuracy=acc, cm=cm)\n",
        "    print(f\"Avg loss: {avg_loss:.4f}  Accuracy: {acc*100:.2f}%\")\n",
        "    print(f\"Confusion matrix:\\n{cm}\")"
      ],
      "metadata": {
        "id": "d0-JY1SlyNtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_-T5vwC-uRm"
      },
      "source": [
        "### Step 6: Modify the Hyperparameters to Optimize Performance of the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for size, res in results.items():\n",
        "    cm = res[\"cm\"]\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(\n",
        "        confusion_matrix=cm,\n",
        "        display_labels=[\"Class 0\", \"Class 1\"]\n",
        "    )\n",
        "    disp.plot(cmap=plt.cm.Blues, values_format=\"d\")\n",
        "    plt.title(f\"Confusion Matrix (dataset_size={size})\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oVLjfVHze1ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# --- config ---\n",
        "LR_GRID   = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
        "EPOCHS    = 100\n",
        "BATCH     = 256\n",
        "DEVICE    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Build your datasets/loaders ONCE (reuse across LRs)\n",
        "train_ds = SUSYDataset(df, dataset_size=50_000, train=True,  high_level_features=None)\n",
        "test_ds  = SUSYDataset(df, dataset_size=50_000, train=False, high_level_features=None)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False)\n",
        "\n",
        "# Replace with your model (must end with Sigmoid for BCELoss)\n",
        "\n",
        "\n",
        "def train_one_lr(lr):\n",
        "    model = neuralNetwork()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "    # --- train ---\n",
        "    model.train()\n",
        "    for _ in tqdm(range(EPOCHS)):\n",
        "        for data, labels in train_loader:\n",
        "            data, labels = data.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            probs = model(data).squeeze(-1)         # already in [0,1]\n",
        "            loss  = criterion(probs, labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # --- eval ---\n",
        "    model.eval()\n",
        "    test_loss, correct, total = 0.0, 0, 0\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data, labels = data.to(DEVICE), labels.to(DEVICE)\n",
        "            probs =\n",
        "            test_loss +=\n",
        "            preds =\n",
        "            correct +=\n",
        "            total +=\n",
        "            all_probs.extend()\n",
        "            all_labels.extend()\n",
        "\n",
        "    avg_loss = test_loss / total\n",
        "    acc = correct / total\n",
        "    cm = confusion_matrix(all_labels, np.array(all_probs) > 0.5, labels=[0,1])\n",
        "\n",
        "    return {\"loss\": avg_loss, \"acc\": acc, \"cm\": cm}\n",
        "\n",
        "# --- run grid ---\n",
        "results = {}\n",
        "for lr in LR_GRID:\n",
        "    print(f\"\\n=== LR = {lr:g} ===\")\n",
        "    res = train_one_lr(lr)\n",
        "    results[lr] = res\n",
        "    print(f\"Avg loss: {res['loss']:.4f}  Acc: {res['acc']*100:.2f}% \")\n",
        "    print(res[\"cm\"])\n"
      ],
      "metadata": {
        "id": "X0arUnsC0nDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- configs ---\n",
        "DATASET_SIZES = [1000, 2000, 5000, 10000]\n",
        "LR_GRID       = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2]\n",
        "EPOCHS        = 5\n",
        "BATCH         = 256\n",
        "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Replace with your model (must output probs in [0,1] for BCELoss)\n",
        "def build_model():\n",
        "    return neuralNetwork()   # <--- your class constructor\n",
        "\n",
        "def run_one(size, lr):\n",
        "    # Build dataset for this size\n",
        "    train_ds = SUSYDataset(df, dataset_size=size, train=True,  high_level_features=None)\n",
        "    test_ds  = SUSYDataset(df, dataset_size=size, train=False, high_level_features=None)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False)\n",
        "\n",
        "    model = build_model().to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "    # --- training ---\n",
        "    model.train()\n",
        "    for _ in range(EPOCHS):\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            probs = model(xb).squeeze(-1)\n",
        "            loss  = criterion(probs, yb.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # --- evaluation ---\n",
        "    model.eval()\n",
        "    test_loss, correct, total = 0.0, 0, 0\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            probs = model(xb).squeeze(-1)\n",
        "            test_loss += criterion(probs, yb.float()).item() * xb.size(0)\n",
        "            preds = (probs > 0.5)\n",
        "            correct += (preds == yb.bool()).sum().item()\n",
        "            total += yb.size(0)\n",
        "            all_probs.extend(probs.detach().cpu().numpy().tolist())\n",
        "            all_labels.extend(yb.detach().cpu().numpy().tolist())\n",
        "\n",
        "    avg_loss = test_loss / total\n",
        "    acc = correct / total\n",
        "\n",
        "    return avg_loss, acc\n",
        "\n",
        "# --- grid search ---\n",
        "results = {}\n",
        "for size in DATASET_SIZES:\n",
        "    for lr in LR_GRID:\n",
        "        print(f\"=== Size {size}, LR {lr:g} ===\")\n",
        "        loss, acc = run_one(size, lr)\n",
        "        results[(size, lr)] = {\"loss\": loss, \"acc\": acc}\n",
        "        print(f\"Loss {loss:.4f}, Acc {acc*100:.2f}%\")\n",
        "\n",
        "# --- make heatmap of accuracy ---\n",
        "acc_matrix = np.zeros((len(DATASET_SIZES), len(LR_GRID)))\n",
        "for i, size in enumerate(DATASET_SIZES):\n",
        "    for j, lr in enumerate(LR_GRID):\n",
        "        acc_matrix[i, j] = results[(size, lr)][\"acc\"] * 100  # percentage\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.heatmap(acc_matrix, annot=True, fmt=\".1f\", xticklabels=LR_GRID, yticklabels=DATASET_SIZES, cmap=\"Blues\")\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Dataset Size\")\n",
        "plt.title(\"Accuracy (%) across Dataset Size × Learning Rate\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1gukF6jw3blc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do you think this is truly OOD?"
      ],
      "metadata": {
        "id": "rvdjpiG8aC7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- configs ---\n",
        "DATASET_SIZES = [] #Pick different hyperparameters\n",
        "LR_GRID       = [] #Pick different hyperparametesr\n",
        "EPOCHS        = 5\n",
        "BATCH         = 256\n",
        "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "def run_one(size, lr):\n",
        "    # Build dataset for this size\n",
        "    train_ds = SUSYDataset(df, dataset_size=size, train=True,  high_level_features=True)\n",
        "    test_ds  = SUSYDataset(df, dataset_size=size, train=False, high_level_features=True)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False)\n",
        "\n",
        "    model = neuralNetwork(high_level_feats=True)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "    # --- training ---\n",
        "    model.train()\n",
        "    for _ in range(EPOCHS):\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            probs = model(xb).squeeze(-1)\n",
        "            loss  = criterion(probs, yb.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # --- evaluation ---\n",
        "    model.eval()\n",
        "    test_loss, correct, total = 0.0, 0, 0\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            probs = model(xb).squeeze(-1)\n",
        "            test_loss += criterion(probs, yb.float()).item() * xb.size(0)\n",
        "            preds = (probs > 0.5)\n",
        "            correct += (preds == yb.bool()).sum().item()\n",
        "            total += yb.size(0)\n",
        "            all_probs.extend(probs.detach().cpu().numpy().tolist())\n",
        "            all_labels.extend(yb.detach().cpu().numpy().tolist())\n",
        "\n",
        "    avg_loss = test_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "# --- grid search ---\n",
        "results = {}\n",
        "for size in DATASET_SIZES:\n",
        "    for lr in LR_GRID:\n",
        "        print(f\"=== Size {size}, LR {lr:g} ===\")\n",
        "        loss, acc = run_one(size, lr)\n",
        "        results[(size, lr)] = {\"loss\": loss, \"acc\": acc}\n",
        "        print(f\"Loss {loss:.4f}, Acc {acc*100:.2f}%\")\n",
        "\n",
        "# --- make heatmap of accuracy ---\n",
        "acc_matrix = np.zeros((len(DATASET_SIZES), len(LR_GRID)))\n",
        "for i, size in enumerate(DATASET_SIZES):\n",
        "    for j, lr in enumerate(LR_GRID):\n",
        "        acc_matrix[i, j] = results[(size, lr)][\"acc\"] * 100  # percentage\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.heatmap(acc_matrix, annot=True, fmt=\".1f\", xticklabels=LR_GRID, yticklabels=DATASET_SIZES, cmap=\"Blues\")\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Dataset Size\")\n",
        "plt.title(\"Accuracy (%) across Dataset Size × Learning Rate\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "raSKwAZa8RLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What do you think should be critical"
      ],
      "metadata": {
        "id": "VF6XitUdzMNC"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}